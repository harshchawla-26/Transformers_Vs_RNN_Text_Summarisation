# Transformers_Vs_RNN_Text_Summarisation

Text summarization is a crucial task in natural language processing that involves generating a concise and coherent summary of a long text. It has numerous applications, including improving information retrieval, enabling faster consumption of large volumes of text, and helping humans to better understand and extract insights from long documents. There are various techniques for performing text summarization, including extractive and abstractive approaches. Extractive summarization involves selecting and aggregating important sentences or phrases from the original text, while abstractive summarization involves generating a new summary by generating novel text that captures the essence of the original document.

In recent years, transformer-based models such as T5 have achieved state-of-the-art performance on a variety of natural language processing tasks, including text summarization. T5 transformers are capable of performing both extractive and abstractive summarization, and have been shown to be highly effective at generating coherent and accurate summaries. On the other hand, Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) that are well-suited for tasks that involve sequential data and long-term dependencies. In this study, we will be comparing the performance of T5 transformers with an LSTM built from scratch on the task of text summarization using the news summary dataset from Kaggle.

This dataset contains a collection of news articles and their corresponding summaries, and provides a useful benchmark for evaluating the performance of text summarization models. We will evaluate the quality of the generated summaries using the ROUGE scores, which are widely used to measure the effectiveness of text summarization models. Our aim is to determine which approach performs better on this task, and to understand the strengths and limitations of each method. By comparing T5 transformers with an LSTM built from scratch on this dataset, we hope to gain insight into the capabilities and limitations of these two approaches for text summarization and to identify areas for future research.
